{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "75c2a0a3-78bb-4fc5-b327-cb4d54c25145",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iniciando processo de ingestão...\nProcessando arquivo: yellow_tripdata_2023-01.parquet\nProcessando arquivo: yellow_tripdata_2023-02.parquet\nProcessando arquivo: yellow_tripdata_2023-03.parquet\nProcessando arquivo: yellow_tripdata_2023-04.parquet\nProcessando arquivo: yellow_tripdata_2023-05.parquet\n\nLeitura e união de todos os arquivos concluída.\n\nAplicando limpeza e filtro de data...\nCriando schema 'ifood_challenge' (se não existir)...\nSalvando dados como tabela gerenciada...\n✅ Tabela 'ifood_challenge.yellow_taxi_trips' criada com sucesso!\n\nVerificando os 10 primeiros registros da tabela criada...\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>vendor_id</th><th>pickup_datetime</th><th>dropoff_datetime</th><th>passenger_count</th><th>total_amount</th></tr></thead><tbody><tr><td>1</td><td>2023-02-01T00:32:53.000Z</td><td>2023-02-01T00:34:34.000Z</td><td>2</td><td>9.4</td></tr><tr><td>2</td><td>2023-02-01T00:35:16.000Z</td><td>2023-02-01T00:35:30.000Z</td><td>1</td><td>5.5</td></tr><tr><td>2</td><td>2023-02-01T00:12:28.000Z</td><td>2023-02-01T00:25:46.000Z</td><td>1</td><td>25.3</td></tr><tr><td>1</td><td>2023-02-01T00:52:40.000Z</td><td>2023-02-01T01:07:18.000Z</td><td>1</td><td>32.25</td></tr><tr><td>1</td><td>2023-02-01T00:12:39.000Z</td><td>2023-02-01T00:40:36.000Z</td><td>1</td><td>50.0</td></tr><tr><td>1</td><td>2023-02-01T00:56:53.000Z</td><td>2023-02-01T01:00:37.000Z</td><td>1</td><td>14.64</td></tr><tr><td>2</td><td>2023-02-01T00:20:40.000Z</td><td>2023-02-01T00:33:56.000Z</td><td>1</td><td>44.12</td></tr><tr><td>2</td><td>2023-02-01T00:33:51.000Z</td><td>2023-02-01T00:37:34.000Z</td><td>1</td><td>12.42</td></tr><tr><td>2</td><td>2023-02-01T01:00:45.000Z</td><td>2023-02-01T01:06:00.000Z</td><td>1</td><td>14.64</td></tr><tr><td>2</td><td>2023-02-01T00:10:48.000Z</td><td>2023-02-01T00:18:09.000Z</td><td>1</td><td>16.0</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         1,
         "2023-02-01T00:32:53.000Z",
         "2023-02-01T00:34:34.000Z",
         2,
         9.4
        ],
        [
         2,
         "2023-02-01T00:35:16.000Z",
         "2023-02-01T00:35:30.000Z",
         1,
         5.5
        ],
        [
         2,
         "2023-02-01T00:12:28.000Z",
         "2023-02-01T00:25:46.000Z",
         1,
         25.3
        ],
        [
         1,
         "2023-02-01T00:52:40.000Z",
         "2023-02-01T01:07:18.000Z",
         1,
         32.25
        ],
        [
         1,
         "2023-02-01T00:12:39.000Z",
         "2023-02-01T00:40:36.000Z",
         1,
         50.0
        ],
        [
         1,
         "2023-02-01T00:56:53.000Z",
         "2023-02-01T01:00:37.000Z",
         1,
         14.64
        ],
        [
         2,
         "2023-02-01T00:20:40.000Z",
         "2023-02-01T00:33:56.000Z",
         1,
         44.12
        ],
        [
         2,
         "2023-02-01T00:33:51.000Z",
         "2023-02-01T00:37:34.000Z",
         1,
         12.42
        ],
        [
         2,
         "2023-02-01T01:00:45.000Z",
         "2023-02-01T01:06:00.000Z",
         1,
         14.64
        ],
        [
         2,
         "2023-02-01T00:10:48.000Z",
         "2023-02-01T00:18:09.000Z",
         1,
         16.0
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "vendor_id",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "pickup_datetime",
         "type": "\"timestamp\""
        },
        {
         "metadata": "{}",
         "name": "dropoff_datetime",
         "type": "\"timestamp\""
        },
        {
         "metadata": "{}",
         "name": "passenger_count",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "total_amount",
         "type": "\"double\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# --- 1. Importação de Bibliotecas ---\n",
    "# Importamos as funções e tipos necessários do PySpark para manipulação de dados.\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.types import IntegerType, DoubleType, TimestampType\n",
    "\n",
    "# --- 2. Definição dos Caminhos ---\n",
    "# Definimos os caminhos para as zonas de landing (dados brutos) e consumo (tabela final).\n",
    "# Utilizamos Volumes do Unity Catalog, que é a prática recomendada em ambientes Databricks modernos.\n",
    "landing_path = \"/Volumes/workspace/default/ifood_landing_zone/\"\n",
    "\n",
    "# --- 3. Leitura, Normalização e União dos Dados ---\n",
    "# Para lidar com as inconsistências de esquema entre os diferentes arquivos mensais,\n",
    "# lemos cada arquivo individualmente em um loop.\n",
    "\n",
    "print(\"Iniciando processo de ingestão...\")\n",
    "\n",
    "files = [f.path for f in dbutils.fs.ls(landing_path) if f.path.endswith(\".parquet\")]\n",
    "final_df = None\n",
    "\n",
    "for file in files:\n",
    "    print(f\"Processando arquivo: {file.split('/')[-1]}\")\n",
    "    \n",
    "    # Lê o arquivo Parquet individualmente\n",
    "    df = spark.read.format(\"parquet\").load(file)\n",
    "    \n",
    "    # Normaliza todos os nomes de colunas para minúsculas para evitar conflitos\n",
    "    for old_name in df.columns:\n",
    "        df = df.withColumnRenamed(old_name, old_name.lower())\n",
    "    \n",
    "    # Seleciona e converte os tipos das colunas necessárias\n",
    "    df = df.select(\n",
    "        col(\"vendorid\").cast(IntegerType()).alias(\"vendor_id\"),\n",
    "        col(\"tpep_pickup_datetime\").cast(TimestampType()).alias(\"pickup_datetime\"),\n",
    "        col(\"tpep_dropoff_datetime\").cast(TimestampType()).alias(\"dropoff_datetime\"),\n",
    "        col(\"passenger_count\").cast(IntegerType()).alias(\"passenger_count\"),\n",
    "        col(\"total_amount\").cast(DoubleType()).alias(\"total_amount\")\n",
    "    )\n",
    "    \n",
    "    # Une o DataFrame processado ao DataFrame final\n",
    "    if final_df is None:\n",
    "        final_df = df\n",
    "    else:\n",
    "        final_df = final_df.unionByName(df)\n",
    "\n",
    "print(\"\\nLeitura e união de todos os arquivos concluída.\")\n",
    "\n",
    "# --- 4. Limpeza e Filtragem dos Dados ---\n",
    "# APLICAMOS AS REGRAS DE NEGÓCIO E GARANTIMOS O ESCOPO DOS DADOS.\n",
    "# ESTA É A ETAPA CORRIGIDA.\n",
    "print(\"\\nAplicando limpeza e filtro de data...\")\n",
    "\n",
    "cleaned_df = final_df.filter(\n",
    "    # Regra 1: Contagem de passageiros deve ser maior que zero.\n",
    "    (col(\"passenger_count\") > 0) &\n",
    "    \n",
    "    # Regra 2: Valor total não pode ser negativo.\n",
    "    (col(\"total_amount\") >= 0) &\n",
    "    \n",
    "    # Regra 3: ID do fornecedor não pode ser nulo.\n",
    "    col(\"vendor_id\").isNotNull() &\n",
    "    \n",
    "    # Regra 4: A data da corrida deve estar estritamente dentro do período solicitado (Jan-Mai 2023).\n",
    "    # ISSO IMPEDE QUE DADOS SUJOS DE OUTROS ANOS OU MESES CONTAMINEM A TABELA FINAL.\n",
    "    (col(\"pickup_datetime\") >= \"2023-01-01\") & (col(\"pickup_datetime\") < \"2023-06-01\")\n",
    "    # **** FIM DA CORREÇÃO ****\n",
    ")\n",
    "\n",
    "# --- 5. Carregamento para a Camada de Consumo ---\n",
    "# Salvamos o DataFrame limpo como uma tabela Delta gerenciada.\n",
    "# O comando .saveAsTable() é a forma mais robusta e compatível com o Unity Catalog,\n",
    "# pois ele gerencia automaticamente a localização física dos dados.\n",
    "\n",
    "print(\"Criando schema 'ifood_challenge' (se não existir)...\")\n",
    "spark.sql(\"CREATE SCHEMA IF NOT EXISTS ifood_challenge\")\n",
    "\n",
    "print(\"Salvando dados como tabela gerenciada...\")\n",
    "cleaned_df.write.format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"overwriteSchema\", \"true\") \\\n",
    "    .saveAsTable(\"ifood_challenge.yellow_taxi_trips\")\n",
    "\n",
    "print(\"✅ Tabela 'ifood_challenge.yellow_taxi_trips' criada com sucesso!\")\n",
    "\n",
    "# --- 6. Verificação ---\n",
    "# Realizamos uma consulta simples para verificar se a tabela foi criada\n",
    "# corretamente e se os dados estão acessíveis via SQL.\n",
    "print(\"\\nVerificando os 10 primeiros registros da tabela criada...\")\n",
    "display(spark.sql(\"SELECT * FROM ifood_challenge.yellow_taxi_trips LIMIT 10\"))\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 8858111818216843,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "ingestao_e_transformacao",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}