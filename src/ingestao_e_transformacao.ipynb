{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "75c2a0a3-78bb-4fc5-b327-cb4d54c25145",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iniciando processo de ingestão...\n\uD83D\uDCC2 Processando arquivo: yellow_tripdata_2023-01.parquet\n\uD83D\uDCC2 Processando arquivo: yellow_tripdata_2023-02.parquet\n\uD83D\uDCC2 Processando arquivo: yellow_tripdata_2023-03.parquet\n\uD83D\uDCC2 Processando arquivo: yellow_tripdata_2023-04.parquet\n\uD83D\uDCC2 Processando arquivo: yellow_tripdata_2023-05.parquet\n\nLeitura e união de todos os arquivos concluída.\n⚙️ Criando schema 'ifood_challenge' (se não existir)...\n\uD83D\uDCBE Salvando dados como tabela gerenciada...\n✅ Tabela 'ifood_challenge.yellow_taxi_trips' criada com sucesso!\n\n\uD83D\uDCCA Verificando os 10 primeiros registros da tabela criada...\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>vendor_id</th><th>pickup_datetime</th><th>dropoff_datetime</th><th>passenger_count</th><th>total_amount</th></tr></thead><tbody><tr><td>2</td><td>2023-01-01T00:32:10.000Z</td><td>2023-01-01T00:40:36.000Z</td><td>1</td><td>14.3</td></tr><tr><td>2</td><td>2023-01-01T00:55:08.000Z</td><td>2023-01-01T01:01:27.000Z</td><td>1</td><td>16.9</td></tr><tr><td>2</td><td>2023-01-01T00:25:04.000Z</td><td>2023-01-01T00:37:49.000Z</td><td>1</td><td>34.9</td></tr><tr><td>2</td><td>2023-01-01T00:10:29.000Z</td><td>2023-01-01T00:21:19.000Z</td><td>1</td><td>19.68</td></tr><tr><td>2</td><td>2023-01-01T00:50:34.000Z</td><td>2023-01-01T01:02:52.000Z</td><td>1</td><td>27.8</td></tr><tr><td>2</td><td>2023-01-01T00:09:22.000Z</td><td>2023-01-01T00:19:49.000Z</td><td>1</td><td>20.52</td></tr><tr><td>2</td><td>2023-01-01T00:27:12.000Z</td><td>2023-01-01T00:49:56.000Z</td><td>1</td><td>64.44</td></tr><tr><td>2</td><td>2023-01-01T00:21:44.000Z</td><td>2023-01-01T00:36:40.000Z</td><td>1</td><td>28.38</td></tr><tr><td>2</td><td>2023-01-01T00:39:42.000Z</td><td>2023-01-01T00:50:36.000Z</td><td>1</td><td>19.9</td></tr><tr><td>2</td><td>2023-01-01T00:53:01.000Z</td><td>2023-01-01T01:01:45.000Z</td><td>1</td><td>19.68</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         2,
         "2023-01-01T00:32:10.000Z",
         "2023-01-01T00:40:36.000Z",
         1,
         14.3
        ],
        [
         2,
         "2023-01-01T00:55:08.000Z",
         "2023-01-01T01:01:27.000Z",
         1,
         16.9
        ],
        [
         2,
         "2023-01-01T00:25:04.000Z",
         "2023-01-01T00:37:49.000Z",
         1,
         34.9
        ],
        [
         2,
         "2023-01-01T00:10:29.000Z",
         "2023-01-01T00:21:19.000Z",
         1,
         19.68
        ],
        [
         2,
         "2023-01-01T00:50:34.000Z",
         "2023-01-01T01:02:52.000Z",
         1,
         27.8
        ],
        [
         2,
         "2023-01-01T00:09:22.000Z",
         "2023-01-01T00:19:49.000Z",
         1,
         20.52
        ],
        [
         2,
         "2023-01-01T00:27:12.000Z",
         "2023-01-01T00:49:56.000Z",
         1,
         64.44
        ],
        [
         2,
         "2023-01-01T00:21:44.000Z",
         "2023-01-01T00:36:40.000Z",
         1,
         28.38
        ],
        [
         2,
         "2023-01-01T00:39:42.000Z",
         "2023-01-01T00:50:36.000Z",
         1,
         19.9
        ],
        [
         2,
         "2023-01-01T00:53:01.000Z",
         "2023-01-01T01:01:45.000Z",
         1,
         19.68
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "vendor_id",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "pickup_datetime",
         "type": "\"timestamp\""
        },
        {
         "metadata": "{}",
         "name": "dropoff_datetime",
         "type": "\"timestamp\""
        },
        {
         "metadata": "{}",
         "name": "passenger_count",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "total_amount",
         "type": "\"double\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# --- 1. Importação de Bibliotecas ---\n",
    "# Importamos as funções e tipos necessários do PySpark para manipulação de dados.\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.types import IntegerType, DoubleType, TimestampType\n",
    "\n",
    "# --- 2. Definição dos Caminhos ---\n",
    "# Definimos os caminhos para as zonas de landing (dados brutos) e consumo (tabela final).\n",
    "# Utilizamos Volumes do Unity Catalog, que é a prática recomendada em ambientes Databricks modernos.\n",
    "landing_path = \"/Volumes/workspace/default/ifood_landing_zone/\"\n",
    "\n",
    "# --- 3. Leitura, Normalização e União dos Dados ---\n",
    "# Para lidar com as inconsistências de esquema entre os diferentes arquivos mensais,\n",
    "# lemos cada arquivo individualmente em um loop.\n",
    "\n",
    "print(\"Iniciando processo de ingestão...\")\n",
    "\n",
    "files = [f.path for f in dbutils.fs.ls(landing_path) if f.path.endswith(\".parquet\")]\n",
    "final_df = None\n",
    "\n",
    "for file in files:\n",
    "    print(f\"\uD83D\uDCC2 Processando arquivo: {file.split('/')[-1]}\")\n",
    "    \n",
    "    # Lê o arquivo Parquet individualmente\n",
    "    df = spark.read.format(\"parquet\").load(file)\n",
    "    \n",
    "    # Normaliza todos os nomes de colunas para minúsculas para evitar conflitos\n",
    "    for old_name in df.columns:\n",
    "        df = df.withColumnRenamed(old_name, old_name.lower())\n",
    "    \n",
    "    # Seleciona e converte os tipos das colunas necessárias\n",
    "    df = df.select(\n",
    "        col(\"vendorid\").cast(IntegerType()).alias(\"vendor_id\"),\n",
    "        col(\"tpep_pickup_datetime\").cast(TimestampType()).alias(\"pickup_datetime\"),\n",
    "        col(\"tpep_dropoff_datetime\").cast(TimestampType()).alias(\"dropoff_datetime\"),\n",
    "        col(\"passenger_count\").cast(IntegerType()).alias(\"passenger_count\"),\n",
    "        col(\"total_amount\").cast(DoubleType()).alias(\"total_amount\")\n",
    "    )\n",
    "    \n",
    "    # Une o DataFrame processado ao DataFrame final\n",
    "    if final_df is None:\n",
    "        final_df = df\n",
    "    else:\n",
    "        final_df = final_df.unionByName(df)\n",
    "\n",
    "print(\"\\nLeitura e união de todos os arquivos concluída.\")\n",
    "\n",
    "# --- 4. Limpeza dos Dados ---\n",
    "# Aplicamos as regras de negócio para garantir a qualidade dos dados,\n",
    "# removendo registros com valores inconsistentes.\n",
    "cleaned_df = final_df.filter(\n",
    "    (col(\"passenger_count\") > 0) &\n",
    "    (col(\"total_amount\") >= 0) &\n",
    "    col(\"vendor_id\").isNotNull()\n",
    ")\n",
    "\n",
    "# --- 5. Carregamento para a Camada de Consumo ---\n",
    "# Salvamos o DataFrame limpo como uma tabela Delta gerenciada.\n",
    "# O comando .saveAsTable() é a forma mais robusta e compatível com o Unity Catalog,\n",
    "# pois ele gerencia automaticamente a localização física dos dados.\n",
    "\n",
    "print(\"⚙️ Criando schema 'ifood_challenge' (se não existir)...\")\n",
    "spark.sql(\"CREATE SCHEMA IF NOT EXISTS ifood_challenge\")\n",
    "\n",
    "print(\"\uD83D\uDCBE Salvando dados como tabela gerenciada...\")\n",
    "cleaned_df.write.format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"overwriteSchema\", \"true\") \\\n",
    "    .saveAsTable(\"ifood_challenge.yellow_taxi_trips\")\n",
    "\n",
    "print(\"✅ Tabela 'ifood_challenge.yellow_taxi_trips' criada com sucesso!\")\n",
    "\n",
    "# --- 6. Verificação ---\n",
    "# Realizamos uma consulta simples para verificar se a tabela foi criada\n",
    "# corretamente e se os dados estão acessíveis via SQL.\n",
    "print(\"\\n\uD83D\uDCCA Verificando os 10 primeiros registros da tabela criada...\")\n",
    "display(spark.sql(\"SELECT * FROM ifood_challenge.yellow_taxi_trips LIMIT 10\"))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 8858111818216843,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "ingestao_e_transformacao",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}